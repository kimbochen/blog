{
  
    
        "post0": {
            "title": "Tesla AI Day - Vision",
            "content": "In this article, I will summarize the Tesla AI Day technical deep dive of Tesla Vision, the vision component of Tesla Autopilot. Moreover, I will try to gather everything referenced in the presentation, such as the mentioned research papers. I also added timestamps of the presentation for each section . . Overview . timestamp . Tesla Vision is a neural network that processes raw images and outputs the vector space. The vector space is a 3-dimensional representation of everything needed for driving, e.g. curbs, traffic signs, and positions of cars. . Neural Network Backbone . Each of the 8 cameras captures 1280 x 960 12-Bit (HDR) images at 36 Hz, which are then fed into a feature extractor. Tesla Vision uses RegNets, a class of ResNets, as feature extractors. . RegNets . timestamp . RegNets offer a nice neural network design space that allows tradeoffs between latency and accuracy. RegNets are proposed in Designing Network Design Spaces by Radosavovic et al. Here’s a high-level description of what the paper is about: . A network design space is defined as a population of model architectures parameterized by certain aspects (e.g. network depth). | The author proposes principles to come up with a design space that contains a high concentration of top-performing models. That is, designing network design spaces. | Concretely, the author progressively simplifies the design space (e.g. setting network depth to a certain number) based on insights gained from the statistics of n sampled models. | The author applied the methodology to ResNets and designed RegNets, which work well under different compute power limits. | . Multi-scale Feature Pyramid Fusion . timestamp . The backbone outputs a number of features at different scales with different purposes: . Resolution Channel counts Neuron behavior . High | Low | Scrutinize image details | . Low | High | See the whole image and have the context | . . A feature pyramid network is then used to process the multi-scale features. The network gets features of different scales to communicate effectively and share information. For instance, the neural network leverages context and details to detect the car. . . Tesla Vision uses BiFPN, proposed in EfficientDet: Scalable and Efficient Object Detection by Tan et al. BiFPN is a feature pyramid network focused on efficiency, and here I summarize its main features: . Efficient Cross-scale Connections: Nodes that have only one input edge are removed based on the intuition that single-input nodes contributes less to feature fusion. | Fast Normalized Feature Fusion: Additional weight for each input is added, which lets the network learn the importance of each feature. To improve training stability, the author proposes a weight normalization approach that is much more efficient than softmax operation. | . Image-to-BEV Transform + Multi-cam Fusion . Driving is done on the vector space, so image space predictions need to be casted into a bird’s eye view (BEV). However, the team faced some challenges when trying to do so. . Challenges . Depth Prediction Inconsistency . timestamp . . Image space predictions look great, but once they are casted into the vector space, predictions look terrible. The reason is that decent vector space predictions require accurate depth predictions in every pixel. What makes it even more difficult is that cameras may be occluded, creating inconsistency in predictions. . Object Detection Inconsistency . timestamp . . When an object spans across multiple cameras (In this case, 5 out of 8 cameras), it is difficult to fuse measurements of multiple images predictions, causing incoherent vector space predicitons. . Variations in Camera Calibration . timestamp . . All of the cars are slightly cockeyed in a different way. For instance, the camera calibration of a pillar camera varies from car to car. The figure above shows the distribution of camera calibrations in the fleet. To do image-to-BEV transforms, camera calibration is needed. . Learning Where to Look . To tackle these problems, the team designed the neural network such that it directly makes predictions in vector space. The neural network needs to do 2 things: . Transform image space features to BEV features. | Fuse information from different cameras, i.e. for every position in the vector space, learn where (in the image space) to look for information. | . Rectifying to a Common Virtual Camera . timestamp . . All of the images are transformed into a common virtual camera using a special rectification transform. This improves the data quality of fused images. For instance, below is the average of all images captured by repeater cameras on a car. . Notice how the back mirror at the upper right gets crisper. . . Regarding the neural network, the architecture used is Transformer, proposed in Attention Is All You Need by Vaswani et al. Specifically, one or more multi-head self-attention blocks is used. . timestamp . . Query Vectors . A raster of the size of the output space is tiled with positional encodings of the output space. Positional encoding is information about the position of every element in the raster, which is represented using sine and cosine functions. These are then encoded with an MLP into a set of query vectors. . Attention . Image features from each camera creates a key and value vector. Keys and queries interact multiplicatively to compute the correlation between every key-query pair. That is, the neural network is learning what features are important / to attend to a given certain query. Here’s an example: . A key vector is created from image features of the pillar camera, containing information about location and what it sees. | A query vector is asking for a certain type of feature at some position of the output space. | . Results . timestamp . After doing all the engineering correctly, which is extremely difficult, the neural network performs drastically better. For instance, there results of curb line detection in the vector space are like night and day. . Object detection is also improved, especially for large objects. Here we can see that the multi-camera setup (blue bounding boxes) predicts the truck a lot more accurately. . Video Neural Net Architecture . Many predictions require not only information of static images but also video context. For example, it is impossible to predict an vehicle’s velocity based on a single image. Thus, the neural network needs memory for remembering what happened in order to deduce new information. . . The team converged on this solution, which includes a feature queue and a video module. . Feature queue caches features over time. | Video module fuses the cached features temporally. | Kinematics, e.g. the car’s own velocity and acceleration, is inserted to keep track of how the car has traveled. | . Feature Queue . timestamp . . Each element in the feature queue is a concatenation of multi-cam features, kinematics, and positional encodings (My interpretation of positional encoding is that it contains the information of the order a feature is pushed). Two types of queues are used based on different rules of when to push. . Time-based queue: Push periodically, e.g. every 27 milliseconds. This gives the neural network the ability to detect occluded objects by referencing previous features. | Space-based queue: Push every time the car travels a certain distance, e.g. every 1 meter. Time-based queue may discard important information that occured a long time ago, e.g. line markings and signs may be forgotten after the car waits for a long red light. Thus, a space-based queue is used to store spatially temporal information. | . Video Module . timestamp . Spatial Recurrent Neural Networks are used to temporally fuse information. . . RNN cells are organized as a 2D lattice, representing the two-dimensional surface we drive on. Hidden states of the cells are updated only when the car is nearby or has visibility. Concretely, as the car is driving around, the kinematics is used to integrate the car’s position into the hidden feature grid. . Here is the visualization of some channels of the hidden feature grid: . . We can see that each channel is keeping track of different aspects of the road, e.g. edges and the road surface. Consider watching the visualization in the presentation video because the visualization is actually dynamic. Here’s the link. . Spatial RNN effectively gives the neural network to selectively read and write information to the memory. . Improvements . Temporary Occlusion . timestamp . The video neural network improved object detection’s robustness to temporary occlusion. . When the car is occluded, the video neural network (blue) retains detection. . When the car is partially occluded, the single-frame neural network (orange) makes terrible predictions based on incomplete information, whereas the video neural network (blue) knows to discard the current occluded scene. . Depth and Velocity . timestamp . Video neural network also improved depth and velocity prediction. . Below is a comparison of depth (upper column) and velocity (lower column) prediction. We can see that the video neural network (blue) performs on par with radar signals (green), while the single-frame network (orange) struggles to predict depth accurately enough to produce decent velocity prediction. . . Multi-task Learning HydraNets . Task-specific Detection Heads . timestamp . The video features are then fed into a number of detection heads, which are one-stage, YOLO-like detectors. A prediction contains: . A raster with 1 bit per position that indicates whether there is an object (e.g. a car) | If there is an object, output its additional attributes (e.g. car type) | . HydraNet . timestamp . The architecture has a common backbone and branches off to multiple detection tasks, which is why it is named HydraNet. HydraNet provides a number of benefits: . Amortized forward pass inference at test time because of feature sharing | De-couples tasks so that tasks can be fine-tuned individually | Backbone features can be cached to disk and used for fine-tuning | A typical training workflow would be like this: . End-to-end training: Train everything jointly | Cache multi-scale features | Fine-tune off of those feature for each task | Recap . timestamp . . Raw images go through rectification layer to transform to a common virtual camera. | Rectified images pass through RegNets to process features at different scales. | Multi-scale features are fused by a BiFPN. | The fused features are fed into a Transformer to rerepresent it in the vector space. | Vector space features are pushed into feature queues based on time and space. | Features in the queues are processed by a spatial RNN video module. | The processed features are passed into the trunks of the HydraNet for detection tasks. | . Improvement Opportunities . timestamp . Late Time and space Fusion . Time and space fusion occurs at nearly the end of inference. Thus, video module and image feature extractors can be replaced with earlier fusion networks, e.g. optical flow networks. Optical flow is the pixel-wise motions between consecutive images. Cost volume is also mentioned. Here’s a basic explanation: . Cost is a measurement of the disparity between the two image pixels at the same location. | When the cost of a pixel in an H x W image is represented as a vector of size L, the cost spans a volume of H x W x L, hence the name. | . Postprocessing Dense Rasters . The neural network outputs are dense rasters, which are costly to postprocess. This is not ideal because the neural network is under strict latency requirements. The team is looking into ways of predicting just the sparse structure of the road. . End Note . I really like how Andrej Karpathy, Sr. Director of AI at Tesla, explains the design choices with problems. In addition, I noticed the emphasis on efficiency: RegNets and BiFPN are all architectures that focus on efficiency. Thank you for reading this long article, I hope you enjoyed it. .",
            "url": "https://kimbochen.github.io/blog/2021/11/09/tesla-ai-day-vision.html",
            "relUrl": "/2021/11/09/tesla-ai-day-vision.html",
            "date": " • Nov 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://kimbochen.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://kimbochen.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, my name is Kimbo Chen, a MS CS student at University of California Riverside. . I have 2 academic goals: . Making deep learning a robust and widely accessible technology. | Accelerate the time and lower the cost of developing computer hardware. | . I am active on Twitter (@kimbochen), where I write about deep learning, computer architecture, and things I learned in general. You can also contact me using email: chentenghung at gmail dot com. . Projects . Experimenting with JAX . A non-trivial example of implementing a deep learning model using JAX. Link to Twitter threads: Testing Phase | Full Project. . It contains a minimal neural network library implemented from scratch using JAX. | A small GPT model is implemented using the library, including the training and evaluating code. | . PanoDPT . An implementation of the paper Dense Prediction Transformer. . Refactored the original implementation and organized the project using PyTorch Lightning. | Designed a 2D scatter operation using Torch Scatter to process panoramic images. | . Systolic Array in PyRTL . Explained how a systolic array works using the register-transfer level design library PyRTL. Link to Twitter thread . Implemented a 2 by 2 systolic array using PyRTL. | Walked through cycle by cycle how the systolic array computes a 2 by 2 matrix multiplication. | . ALREC: Army Leave Report Chatbot . A chatbot I wrote when serving in the military to deal with the incoveniences of reporting status. Link to Twitter thread . Deployed a LINE chatbot on hosting service Heroku using Google Sheets as backend. | Streamlined the reporting process of a 15-person message group. | . Research Interest . I am interested in machine learning systems, MLOps, and computer architecture in general. Here are some papers I read and summarized: . Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks | VOS: Learning What You Don’t Know by Virtual Outlier Synthesis | Planaria: Dynamic Architecture Fission for Spatial Multi-Tenant Acceleration of Deep Neural Networks | Dark Silicon and the End of Multi-core Scaling | . I also covered many parts of the Tesla AI Day: . Vision | Auto Labeling | Tesla Dojo Computer | . Community Experiences . I attended many online events to meet other researchers and learn new things. After attending, I wrote down what I learned. Here are some highlights: . ICCV Undergraduates in Computer Vision | Computer Architecture Long-term Mentoring | Seminar Series on Tensor Computation - Halide | Big Science Data Sourcing Sprint | Machine Learning Tokyo Session - ResNet Strikes Back | PyMC Open Source Sprint | . Education . B.A. in Computer Science, National Tsing Hua University Sept. 2017 - Jun. 2021 | Advisor: Hwann-Tzong Chen | . | .",
          "url": "https://kimbochen.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kimbochen.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}